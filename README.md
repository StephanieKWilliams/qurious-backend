
ğŸ§  AI Chat Backend â€“ FastAPI API for AI-Powered Conversations

A powerful and minimal FastAPI backend that powers a modern AI chat frontend. Designed for speed, clarity, and modular AI integration with Groqâ€™s ultra-fast inference API.

    ğŸ’¡ The frontend (built with Next.js 15) is separately hosted and connects via API.

ğŸš€ Features

    ğŸ§  Groq AI Integration â€“ Ultra-fast inference with LLaMA & Mixtral models

    âš¡ FastAPI â€“ Blazing-fast Python backend with Pydantic validation

    ğŸ§ª API Testing â€“ Built-in /health and / endpoints

    ğŸ§¾ OpenAPI Docs â€“ Auto-generated Swagger & ReDoc documentation

    ğŸ” Secure Key Handling â€“ Groq key managed via .env

    ğŸ”„ CORS-Ready â€“ For frontend-to-backend browser requests

ğŸ›  Tech Stack

    FastAPI â€“ Python web framework

    Groq API â€“ LLM backend

    Uvicorn â€“ ASGI server

    HTTPX â€“ Async HTTP client

    Pydantic â€“ Data validation and serialization

    python-dotenv â€“ Environment configuration

ğŸ“¦ Installation
Prerequisites

    Python 3.8+

    A valid Groq API Key from https://console.groq.com

Clone & Setup

git clone <this-repo-url>
cd backend

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

âš™ï¸ Environment Variables

Create a .env file in the root of backend/ directory.

GROQ_API_KEY=gsk_your_groq_api_key_here
DEBUG=True
LOG_LEVEL=INFO
ALLOWED_ORIGINS=https://qurious-seven.vercel.app

    ALLOWED_ORIGINS must match your deployed frontend domain.

ğŸš€ Running the Backend

uvicorn main:app --host 0.0.0.0 --port 8000 --reload

or

python main.py

Your FastAPI server will be available at:

http://localhost:8000

ğŸ”§ API Endpoints
GET /

    âœ… Health check root

    â„¹ï¸ Returns basic server info

GET /health

    âœ… Full backend health diagnostics

    Returns system status and model readiness

GET /api/models

    ğŸ“œ Lists supported LLM models

POST /api/chat

    ğŸ¤– Sends user message and conversation history to LLM


ğŸ“š Supported Models

    llama-3.1-8b-instant (default)

    llama-3.1-70b-versatile

    mixtral-8x7b-32768

    gemma2-9b-it

ğŸ§ª Testing the API
Health Check

curl http://localhost:8000/health

Chat Endpoint

curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Hello","history":[]}'

